import random
import torch
from transformers import GPT2LMHeadModel,  GPT2Tokenizer
import nltk

import os

MODEL_DIR=os.path.join("gptV1","model_save")

# Contains the prototyping for executing gpt to generate data

def execute_gpt(prompt : list, num_return_sequences : int, gen_json: bool = False):
	model = GPT2LMHeadModel.from_pretrained(MODEL_DIR)
	tokenizer = GPT2Tokenizer.from_pretrained(MODEL_DIR)

	model.eval()
	results = []

	for p in prompt:
		generated = torch.tensor(tokenizer.encode(p)).unsqueeze(0)

		sample_outputs = model.generate(
			generated, 
			bos_token_id=random.randint(1,30000),
			do_sample=True,   
			top_k=50, 
			max_length = 500,
			top_p=0.95, 
			num_return_sequences=num_return_sequences,
			pad_token_id = tokenizer.eos_token_id 
		)

		decoded_sample_outputs = []

		for sample_output in sample_outputs:
			decoded_sample_outputs.append(tokenizer.decode(sample_output, skip_special_tokens=True))
		
		if gen_json == True:
			gen_results = []
			for d in decoded_sample_outputs:
				gen_results.append(["GPT", p, d])
			results.append(gen_results)
		else:
			for d in decoded_sample_outputs:
				results.append(d)

	return results

# Method with adjusted output format for the experiment, the combined-parameter enables the functionality to process data from another model.

def execute_gpt_experiment(prompt : list, num_return_sequences : int, combined:bool = False):
	model = GPT2LMHeadModel.from_pretrained(MODEL_DIR)
	tokenizer = GPT2Tokenizer.from_pretrained(MODEL_DIR)

	model.eval()
	results = []

	if combined == False:

		for p in prompt:
			words = nltk.word_tokenize(p)
			# as opposed to execute_gpt, the prompts are generated by splitting the input-sentences into halfs. 
			gpt_sample = (" ".join(words[0:int((len(words)/2))]))

			generated = torch.tensor(tokenizer.encode(gpt_sample)).unsqueeze(0)

			sample_outputs = model.generate(
				generated, 
				bos_token_id=random.randint(1,30000),
				do_sample=True,   
				top_k=50, 
				max_length = 500,
				top_p=0.95, 
				num_return_sequences=num_return_sequences,
				pad_token_id = tokenizer.eos_token_id 
			)

			decoded_sample_outputs = []

			for sample_output in sample_outputs:
				decoded_sample_outputs.append(tokenizer.decode(sample_output, skip_special_tokens=True))
			
			gen_results = []

			for d in decoded_sample_outputs:
				# For the output, the input-sentences are used as reference prompts. 
				if d is None:
					continue
				else:
					gen_results.append(["GPT", p, d])

			results += gen_results
	
	else:
		for p in prompt:
			words = nltk.word_tokenize(p[2])
			# as opposed to execute_gpt, the prompts are generated by splitting the input-sentences into halfs. 
			gpt_sample = (" ".join(words[0:int((len(words)/2))]))

			generated = torch.tensor(tokenizer.encode(gpt_sample)).unsqueeze(0)

			sample_outputs = model.generate(
				generated, 
				bos_token_id=random.randint(1,30000),
				do_sample=True,   
				top_k=50, 
				max_length = 500,
				top_p=0.95, 
				num_return_sequences=num_return_sequences,
				pad_token_id = tokenizer.eos_token_id 
			)

			decoded_sample_outputs = []

			for sample_output in sample_outputs:
				decoded_sample_outputs.append(tokenizer.decode(sample_output, skip_special_tokens=True))
			
			gen_results = []

			for d in decoded_sample_outputs:
				# For the output, the input-sentences are used as reference prompts. 
				if d is None:
					continue
				else:
					gen_results.append(["GPT", p[1], d])

			results += gen_results

	return results


